nohup python selfplay.py >> log.txt 2>&1 &

# 问题

## 1. 每一步策略迭代的时候，应该拿怎样的训练数据去训练策略网络和价值网络。

### 现在的程序：

每一次 ai 自我对弈，在对弈的过程中，每走一步棋之前先进行 numMCTSSims=500 次 MCTS 搜索，再获取当前状态 $s$ 的所有子节点的探索次数作为新的动作概率分布 $p$，各个子节点的历史经验 $\overline{Q}$ 与 $p$ 做加权求和得到当前状态估计的价值 $v$。因此一盘棋，每走一步获得 $(s, p, v)$ 三元组，再经过对称性变换获得总共 8 个三元组。获取完训练数据后，按照概率分布 $p$ 采样下棋，然后继续 "MCTS 搜索 -> 获取训练数据 -> 采样下棋" 循环，直到游戏结束。
下 $N*100$ 次棋，总共大约获得 $N*100*49*8$ 个训练数据。

### 开源程序 https://github.com/suragnair/alpha-zero-general.git ：
与现在的程序不同的是， $v$ 取为 $1,-1$ 两种可能。如果这一轮自我对弈赢了就是 $1$，否则就是 $-1$。

### 问题
获得的训练数据去训练神经网络后，有时效果反而更差了，这是为什么？ 怎样能保证这个策略迭代的过程能够有效地收敛至最优策略？

## 2. 如何判断网络迭代训练的好不好，有没有什么可以量化的指标

### 现在的程序：
每次获取一定数量的训练数据（100000 量级）后，去训练策略网络和价值网络，用得到的新网络和旧网络进行对弈（50 次游戏），如果胜率超过 55% 就承认这次迭代是有效的，拿它替换旧网络；否则 reject 新网络。

### 问题：
实际测试发现除了一开始几次 accept，后来每次都是 reject。问题是否还是出在训练数据不够好导致策略迭代不成功？

除了这种竞技场对弈评价网络好坏的方式，还有没有其他可以量化的指标？

## 3. 如何通过尽量少的代价获得一个比较强大的 AI？如何事先去预估时间和算力代价，而不是浪费太多的时间在各种调整超参数和 debug 上面。

现在的程序有许多超参数 （在 selfplay.py 开头），
还有一些神经网络训练的超参数没有包括进去 (lr, weightdecay 之类的)。
而过去的一段时间里以为人工调整这些参数能够获得好的效果，但是都失败了，反而浪费了很长的时间。这个事情有没有什么理论指导可以少走弯路。

## 4. 强化学习有没有什么比较好的学习路径，感觉自己有非常多的困惑，想寻找一个比较坚实可靠的理论基础
