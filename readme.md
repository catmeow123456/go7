nohup python selfplay.py >> log.txt 2>&1 &

# 问题

## 1. 每一步策略迭代的时候，应该拿怎样的训练数据去训练策略网络和价值网络。

### 现在的程序：

每一次 ai 自我对弈，在对弈的过程中，每走一步棋之前先进行 numMCTSSims=500 次 MCTS 搜索，再获取当前状态 $s$ 的所有子节点的探索次数作为新的动作概率分布 $p$，各个子节点的历史经验 $\overline{Q}$ 与 $p$ 做加权求和得到当前状态估计的价值 $v$。因此一盘棋，每走一步获得 $(s, p, v)$ 三元组，再经过对称性变换获得总共 8 个三元组。获取完训练数据后，按照概率分布 $p$ 采样下棋，然后继续 "MCTS 搜索 -> 获取训练数据 -> 采样下棋" 循环，直到游戏结束。
下 $N*100$ 次棋，总共大约获得 $N*100*49*8$ 个训练数据。

### 开源程序 https://github.com/suragnair/alpha-zero-general.git ：
与现在的程序不同的是， $v$ 取为 $1,-1$ 两种可能。如果这一轮自我对弈赢了就是 $1$，否则就是 $-1$。

### 问题
获得的训练数据去训练神经网络后，有时效果反而更差了，这是为什么？ 怎样能保证这个策略迭代的过程能够有效地收敛至最优策略？

## 2. 如何判断网络迭代训练的好不好，有没有什么可以量化的指标

### 现在的程序：
每次获取一定数量的训练数据（100000 量级）后，去训练策略网络和价值网络，用得到的新网络和旧网络进行对弈（50 次游戏），如果胜率超过 55% 就承认这次迭代是有效的，拿它替换旧网络；否则 reject 新网络。

### 问题：
实际测试发现经常 reject 新网络，问题可能出在训练数据不够好导致策略迭代不成功。这样的策略迭代过程很费时间，通常一天只能迭代 2-3 次。

除了这种竞技场对弈评价网络好坏的方式，还有没有其他可以量化的指标？

## 3. 如何通过尽量少的代价获得一个比较强大的 AI？如何事先去预估时间和算力代价，而不是浪费太多的时间在各种调整超参数和 debug 上面。

现在的程序有许多超参数 （在 selfplay.py 开头），
还有一些神经网络训练的超参数没有包括进去 (lr, weightdecay 之类的)。
而过去的一段时间里以为人工调整这些参数能够获得好的效果，但是都失败了，反而浪费了很长的时间。这个事情有没有什么理论指导可以少走弯路。

## 4. 强化学习有没有什么比较好的学习路径，感觉自己有非常多的困惑，想寻找一个比较坚实可靠的理论基础

## 5. alphago 采用的 PUCT 公式是怎么来的，有什么依据
为什么 alphago 采用的带有策略先验的 PUCT 公式是
$$
u(s,a) = \bar{Q}_{s,a} + c_\mathrm{puct} P(s,a) \frac{\sum_b N(s,b)}{1 + N(s,a)}
$$
而一般的 UCT 公式是
$$
u(s,a)=\bar{Q}_{s,a} + c_\mathrm{uct} \sqrt{\frac{2\ln t}{N(s,a)}}
$$
这两个公式好像完全不一样？这些公式是怎么被写出来的？

在面临具体的强化学习问题时，怎样去权衡"探索"和"经验"，哪些情况下只能用 $\epsilon$-greedy，哪些情况能用 UCT 公式？

## 6. mcts 收集数据的过程太费时间了，有没有什么办法可以并行加快？

之前尝试过用 python 的 concurrent.futures 模块去同时执行多次 selfplay，但是却发现没有得到加速，原因可能是用神经网络去 predict 的过程没有真正地并行。

有没有什么比较好的并行方法，比如在 mcts 搜索的过程中，等待收到一定数量个 predict request 后，将这些 requests 的输入合并为一个 tensor 一起做 predict，在 predict 结束之前这些 mcts 线程被锁死，直到 predict 结束以后将结果返还给各个线程。这样做能有效加速 selfplay 过程吗？这样的代码应该怎样写？